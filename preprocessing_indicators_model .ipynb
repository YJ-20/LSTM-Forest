{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2dAkL0bSEnOT"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DIu0Ui5yJ738"
   },
   "outputs": [],
   "source": [
    "raw_data=pd.read_excel('엑셀파일') # 엑셀파일 불러와서 정리안된 아래 기술지표함수 참고하여 추가  (DX, ADX, MFI, williams r, OBV 등 몇개 아직 미완료 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rT7fFDJE28j"
   },
   "outputs": [],
   "source": [
    "# Data preprocessing 기술지표관련 \n",
    "# volumne variance 큰것 분류해서 따로 분석필요\n",
    "\n",
    "def ADX(data, period):  \n",
    "    df = data[['Close','High','Low','TR']].copy()\n",
    "    alpha = 1/period\n",
    "    df['ATR'] = df['TR'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "    # +-DX\n",
    "    df['H-pH'] = df['High'] - df['High'].shift(1)\n",
    "    df['pL-L'] = df['Low'].shift(1) - df['Low']\n",
    "    df['+DX'] = np.where(\n",
    "        (df['H-pH'] > df['pL-L']) & (df['H-pH']>0),\n",
    "        df['H-pH'], 0.0    )\n",
    "    df['-DX'] = np.where(\n",
    "        (df['H-pH'] < df['pL-L']) & (df['pL-L']>0),\n",
    "        df['pL-L'], 0.0    )\n",
    "    del df['H-pH'], df['pL-L']\n",
    "\n",
    "    # +- DMI\n",
    "    df['S+DM'] = df['+DX'].ewm(alpha=alpha, adjust=False).mean()\n",
    "    df['S-DM'] = df['-DX'].ewm(alpha=alpha, adjust=False).mean()\n",
    "    df['+DMI'] = (df['S+DM']/df['ATR'])*100\n",
    "    df['-DMI'] = (df['S-DM']/df['ATR'])*100\n",
    "    del df['S+DM'], df['S-DM']\n",
    "\n",
    "    # ADX\n",
    "    df['DX'] = (np.abs(df['+DMI'] - df['-DMI'])/(df['+DMI'] + df['-DMI']))*100\n",
    "    df['ADX'] = df['DX'].ewm(alpha=alpha, adjust=False).mean()\n",
    "    del df['ATR'], df['-DX'], df['+DX'], df['+DMI'], df['-DMI']\n",
    "    return df['DX'],df['ADX']\n",
    "\n",
    "def rsi_n(data,n):\n",
    "  delta=np.array(data.diff(1))\n",
    "  delta[np.isnan(delta)]=0.000000001\n",
    "  dUp, dDown = delta.copy(), delta.copy()\n",
    "  dUp[dUp < 0] = 0\n",
    "  dDown[dDown > 0] = 0\n",
    "  RolUp = pd.DataFrame(dUp).rolling(n).mean()\n",
    "  RolDown = pd.DataFrame(dDown).abs().rolling(n).mean()\n",
    "  RS =RolUp / RolDown\n",
    "  rsi= 100.0 - (100.0 / (1.0 + np.array(RS)))\n",
    "  return rsi\n",
    "\n",
    "def forceindex(data_c,data_v,ndays):\n",
    "    ForceIndex=pd.Series(data_c.diff(1)* data_v)    \n",
    "    return ForceIndex.ewm(ndays).mean()\n",
    "\n",
    "\n",
    "def CCI(data_tp, ndays): \n",
    "    CCI = pd.Series((data_tp - data_tp.rolling(ndays).mean()) / (0.015 * data_tp.rolling(ndays).std()),name = 'CCI') \n",
    "    return CCI \n",
    "\n",
    "def money_flow_index(df, periods=14):\n",
    "    data=df.copy()\n",
    "    data['money_flow'] = np.array(data['TP']) * np.array(data['Volume'])\n",
    "    # data['money_flow'] = data['TP'] * data['Volume']\n",
    "    delta=data['money_flow'].diff(1)\n",
    "    delta[np.isnan(delta)]=0.000000001\n",
    "    dUp, dDown = delta.copy(), delta.copy()\n",
    "    dUp[dUp < 0] = 0\n",
    "    dDown[dDown > 0] = 0\n",
    "    RolUp = pd.DataFrame(dUp).rolling(periods).mean()\n",
    "    RolDown = pd.DataFrame(dDown).abs().rolling(periods).mean()\n",
    "    MFR =RolUp / RolDown\n",
    "    MFI= 100.0 - (100.0 / (1.0 + np.array(MFR)))\n",
    "    return MFR, MFI\n",
    "\n",
    "def negative_volume_index(data, periods=255, close_col='<CLOSE>', vol_col='<VOL>'):\n",
    "    data['nvi'] = 0.\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        if index > 0:\n",
    "            prev_nvi = data.at[index-1, 'nvi']\n",
    "            prev_close = data.at[index-1, close_col]\n",
    "            if row[vol_col] < data.at[index-1, vol_col]:\n",
    "                nvi = prev_nvi + (row[close_col] - prev_close / prev_close * prev_nvi)\n",
    "            else: \n",
    "                nvi = prev_nvi\n",
    "        else:\n",
    "            nvi = 1000\n",
    "        data.set_value(index, 'nvi', nvi)\n",
    "    data['nvi_ema'] = data['nvi'].ewm(ignore_na=False, min_periods=0, com=periods, adjust=True).mean()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def williams_r(data, periods=14, high_col='<HIGH>', low_col='<LOW>', close_col='<CLOSE>'):\n",
    "    data['williams_r'] = 0.\n",
    "    \n",
    "    for index,row in data.iterrows():\n",
    "        if index > periods:\n",
    "            data.set_value(index, 'williams_r', ((max(data[high_col][index-periods:index]) - row[close_col]) / \n",
    "                                                 (max(data[high_col][index-periods:index]) - min(data[low_col][index-periods:index]))))\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def on_balance_volume(data, trend_periods=21, close_col='<CLOSE>', vol_col='<VOL>'):\n",
    "    for index, row in data.iterrows():\n",
    "        if index > 0:\n",
    "            last_obv = data.at[index - 1, 'obv']\n",
    "            if row[close_col] > data.at[index - 1, close_col]:\n",
    "                current_obv = last_obv + row[vol_col]\n",
    "            elif row[close_col] < data.at[index - 1, close_col]:\n",
    "                current_obv = last_obv - row[vol_col]\n",
    "            else:\n",
    "                current_obv = last_obv\n",
    "        else:\n",
    "            last_obv = 0\n",
    "            current_obv = row[vol_col]\n",
    "\n",
    "        data.set_value(index, 'obv', current_obv)\n",
    "\n",
    "    data['obv_ema' + str(trend_periods)] = data['obv'].ewm(ignore_na=False, min_periods=0, com=trend_periods, adjust=True).mean()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "class EaseOfMovementIndicator(IndicatorMixin):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        high: pd.Series,\n",
    "        low: pd.Series,\n",
    "        volume: pd.Series,\n",
    "        window: int = 14,\n",
    "        fillna: bool = False,\n",
    "    ):\n",
    "        self._high = high\n",
    "        self._low = low\n",
    "        self._volume = volume\n",
    "        self._window = window\n",
    "        self._fillna = fillna\n",
    "        self._run()\n",
    "\n",
    "    def _run(self):\n",
    "        self._emv = (\n",
    "            (self._high.diff(1) + self._low.diff(1))\n",
    "            * (self._high - self._low)\n",
    "            / (2 * self._volume)\n",
    "        )\n",
    "        self._emv *= 100000000\n",
    "\n",
    "    def ease_of_movement(self) -> pd.Series:\n",
    "        \"\"\"Ease of movement (EoM, EMV)\n",
    "        Returns:\n",
    "            pandas.Series: New feature generated.\n",
    "        \"\"\"\n",
    "        emv = self._check_fillna(self._emv, value=0)\n",
    "        return pd.Series(emv, name=f\"eom_{self._window}\")\n",
    "\n",
    "    def sma_ease_of_movement(self) -> pd.Series:\n",
    "        \"\"\"Signal Ease of movement (EoM, EMV)\n",
    "        Returns:\n",
    "            pandas.Series: New feature generated.\n",
    "        \"\"\"\n",
    "        min_periods = 0 if self._fillna else self._window\n",
    "        emv = self._emv.rolling(self._window, min_periods=min_periods).mean()\n",
    "        emv = self._check_fillna(emv, value=0)\n",
    "        return pd.Series(emv, name=f\"sma_eom_{self._window}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mE3hjWchE2_M"
   },
   "outputs": [],
   "source": [
    "# 2000-01-03 / 2019-07-01\n",
    "# 2007-01-03 / 2011-12-30\n",
    "## target논문 데이터 기간\n",
    "\n",
    "\n",
    "train_x=snp_x.loc['2007-01-03':'2011-01-02']\n",
    "test_x=snp_x.loc['2011-01-03':'2011-12-30']\n",
    "\n",
    "train_y=snp_y.loc['2007-01-03':'2011-01-02'] ## 논문에서 제시한 train기간\n",
    "test_y=snp_y.loc['2011-01-03':'2011-12-30']  ## 논문에서 제시한 test기간\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7GKthTFBE3BN"
   },
   "outputs": [],
   "source": [
    "#scaler=StandardScaler()\n",
    "scaler=MinMaxScaler()  # 0~1 nomarlization (input전에 전체데이터 전처리)\n",
    "scaler.fit(np.array(train_x))\n",
    "\n",
    "train_x_n=scaler.transform(train_x)\n",
    "test_y_n=scaler.transform(test_x)\n",
    "\n",
    "\n",
    "train_y_return=train_y['ln_return']\n",
    "train_y_class=to_categorical(train_y['up_down'])\n",
    "\n",
    "test_y_return=test_y['ln_return']\n",
    "test_y_class=to_categorical(test_y['up_down'])\n",
    "\n",
    "\n",
    "##### total\n",
    "data_y=pd.concat([train_y,test_y])\n",
    "data_x=pd.concat([train_x,test_x])\n",
    "\n",
    "data_x_n=scaler.transform(data_x)\n",
    "\n",
    "data_y_return=np.array(data_y['ln_return'])\n",
    "data_y_class=to_categorical(data_y['up_down'])\n",
    "\n",
    "\n",
    "\n",
    "train_section=len(test_x)\n",
    "print(len(data_x_n),len(train_x),len(test_x))\n",
    "print(test_y_return.shape,train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yCkSjkQwE3De"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee5LazsYE3F3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "APOJYZCAE3IW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR4StCxZE3Kq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "71NXwrBbE3M3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEjO3LGgE3Pb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwMCCeWIE3Rw"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import random, sys, random, pickle\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, LSTM, Dense, Dropout\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.layers import Reshape, Activation, Conv2D, MaxPooling2D, BatchNormalization, Flatten, Lambda\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint,LearningRateScheduler, TensorBoard\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.layers.merge import concatenate\n",
    "import keras.backend as K\n",
    "from pandas_datareader import data as pdr\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "\n",
    "\n",
    "def create_timestep(dataset,n):\n",
    "    dataX= []\n",
    "    for i in range(len(dataset)-n+1):\n",
    "        a = dataset[i:(i+n)]\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "def count(data):  \n",
    "    c0=0\n",
    "    c1=0\n",
    "    for i in range(len(data)):\n",
    "        if np.argmax(data[i])==0:\n",
    "            c0=c0+1\n",
    "        elif np.argmax(data[i])==1:\n",
    "            c1=c1+1\n",
    "    return c0,c1\n",
    "def label(data):\n",
    "    k=[]\n",
    "    for i in range(len(data)):\n",
    "        if np.argmax(data[i])==0:\n",
    "            k.append(0)\n",
    "        elif np.argmax(data[i])==1:\n",
    "            k.append(1)\n",
    "    return np.array(k)\n",
    "\n",
    "\n",
    "drop_rate=0.5\n",
    "initial_rate=0.001\n",
    "step_epoch=50\n",
    "lrList = []\n",
    "def step_decay(epoch):\n",
    "    lrate = initial_rate\n",
    "    if epoch >= step_epoch:\n",
    "        lrate = initial_rate*drop_rate\n",
    "    elif epoch >=step_epoch*2:\n",
    "        lrate = lrate*drop_rate\n",
    "    elif epoch >=step_epoch*3:\n",
    "        lrate = lrate*drop_rate\n",
    "    lrList.append(lrate)\n",
    "    return lrate\n",
    "\n",
    "\n",
    "\n",
    "def lstm_forest_model(var,index,data,seq):\n",
    "  for i in range(var):\n",
    "    if i==0:\n",
    "      lf_datax=create_timestep(data[:,index[0]],seq)      \n",
    "      lf_datax=lf_datax.reshape((-1,seq,1))      \n",
    "    else:\n",
    "      lf_datax_i=create_timestep(data[:,index[i]],seq).reshape((-1,seq,1))      \n",
    "      lf_datax=np.concatenate((lf_datax,lf_datax_i),axis=2)           \n",
    "  return lf_datax\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def test_LFM(data_x,y_return,y_cross,train_section):\n",
    "  \n",
    "  var=13\n",
    "  seq=50\n",
    "  epoch=150\n",
    "\n",
    "  lis=range(data_x.shape[1])  \n",
    "  index=random.sample(lis,var)    \n",
    "  print('index',index)\n",
    "  lf_datax=lstm_forest_model(var,index,data_x,seq)  \n",
    "  lr = LearningRateScheduler(step_decay)\n",
    "  callbacks_list = [lr]\n",
    "  section=train_section\n",
    "  \n",
    "  y_cross, y_return =y_cross[seq-1:], y_return[seq-1:]\n",
    "  train_x2, test_x = lf_datax[:-section,:,:], lf_datax[-section:,:,:]\n",
    "  train_y_cross2, test_y_cross=y_cross[:-section],y_cross[-section:]\n",
    "  train_y_return2, test_y_return=y_return[:-section],y_return[-section:]  \n",
    "  \n",
    "  #train sample random choice\n",
    "  index_random2=random.sample(range(train_x2.shape[0]),int(train_x2.shape[0]*7/8))  \n",
    "  index_random=np.sort(index_random2)\n",
    "\n",
    "  train_x=train_x2[index_random,:,:]    \n",
    "  train_y_cross=train_y_cross2[index_random,:]    \n",
    "  train_y_return=train_y_return2[index_random]\n",
    "  # train_y_return=train_y_return2[index_random,:]\n",
    "  \n",
    "  # modeling\n",
    "  K.clear_session() \n",
    "  main_input = Input(shape=(seq,var),name='main_input')\n",
    "  m=LSTM(15,return_sequences=False)(main_input)\n",
    "  \n",
    "  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)\n",
    "  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)\n",
    "  m=Dense(40,activation='relu',kernel_initializer='he_normal')(m)\n",
    "  m=Dropout(0.3)(m)\n",
    "\n",
    "  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(m)\n",
    "  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)\n",
    "  main_cross=Dropout(0.3)(m)\n",
    "  main_cross=Dense(30,activation='relu',kernel_initializer='he_normal')(main_cross)\n",
    "  main_cross=Dense(2,activation='softmax',name='main_cross')(main_cross)\n",
    "\n",
    "  main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(m)\n",
    "  main_return=Dense(30,activation='relu',kernel_initializer='he_normal')(main_return)\n",
    "  main_return=Dropout(0.3)(main_return)\n",
    "  main_return=Dense(30,kernel_initializer='he_normal')(main_return)\n",
    "  main_return=Dense(1,activation='linear',name='main_return')(main_return)\n",
    "  # model.summary()\n",
    "  model=Model(inputs=main_input,outputs=[main_cross,main_return])\n",
    "  model.compile(optimizer='adam',loss={'main_cross':'categorical_crossentropy','main_return':'mean_squared_error'},\n",
    "                  metrics=['accuracy'],loss_weights={'main_cross':8,'main_return':1})\n",
    "  print(\"model : \",model.summary())\n",
    "  history=model.fit({'main_input':train_x},{'main_cross':train_y_cross,'main_return':train_y_return},shuffle=False,callbacks=callbacks_list,\n",
    "                    validation_split=1/8,epochs=epoch,batch_size=256,verbose=0)\n",
    "  \n",
    "  # model evaluation\n",
    "  lfm_predict=model.predict(test_x)\n",
    "\n",
    "  val_section=int(len(train_x)*1/8)\n",
    "  val_result=model.evaluate(train_x[-val_section:],[train_y_cross[-val_section:],train_y_return[-val_section:]])\n",
    "  print(\"test evaluate\",model.evaluate(test_x,[test_y_cross,test_y_return]))\n",
    "  print(len(lfm_predict[0]),len(lfm_predict[1]))\n",
    "  model.save('save dir + stockanme+ num_var+ sequence+ -th lstm')  ## ex :' /user/hyunjun/asc/lstm_forest/lfm/snp_v13_seq50_lstm{}' .format(p)    p= 아래 몇번째 LSTM 인지 \n",
    "  \n",
    "  return lfm_predict,val_result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lfm_var13_se50=[]\n",
    "\n",
    "import pickle\n",
    "for p in range(103):\n",
    "  print(p ,'-th complete')\n",
    "  lfm_var13_se50.append(test_LFM(data_x_n,data_y_return,data_y_class,train_section))            \n",
    "\n",
    "# 최종 predidction : 100개의 lfm model로부터 prediction하고 예측값 앙상블한결과\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A490gMLBE3UD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aqCtE9F9E3Wg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NCL0ssLE3Y-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Er_ZAl2uE3bO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ztoYaMQE3dt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nLVuPBEwE3gO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPdKWhcXf6cGRIWHY8qhCu8",
   "collapsed_sections": [],
   "name": "기술지표관련함수.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
